#' Prune out low-quality assignments
#'
#' Remove low-quality assignments based on the cell-label score matrix returned by \code{\link{classifySingleR}}.
#'
#' @param results A \linkS4class{DataFrame} containing the output generated by \code{\link{SingleR}} or \code{\link{classifySingleR}}.
#' @param min.diff.med Numeric scalar specifying the minimum difference of each cell's maximum score from the median score.
#' @param min.diff.next Numeric scalar specifying the minimum difference between the best score and the next best score in fine-tuning.
#' @param nmads Numeric scalar specifying the number of MADs to use to define cells with low outlier scores per label.
#' 
#' @return A logical vector specifying which assignments should be ignored.
#'
#' @details
#' By itself, the SingleR algorithm will always assign a label to every cell.
#' This occurs even if the cell's true label is not represented in the reference set of labels,
#' resulting in assignment of an incorrect label to that cell.
#' The \code{pruneScores} function aims to mitigate this effect by removing poor-quality assignments with \dQuote{low} scores.
#' We define a low score using several orthogonal measures that operate on a per-cell or per-label basis, as described below.
#'
#' The first per-cell check occurs for each cell using the scores in \code{results$scores} prior to fine-tuning.
#' Here, we see whether the maximum score is less than \code{min.diff.med} above the median score for each cell.
#' If so, this indicates that the cell matches all labels with the same confidence 
#' such that the one reported label is not particularly meaningful.
#' This can also be justified in high-dimensional analyses where,
#' in the absence of any strong similarity to a single label, all distances converge to the same value.
#' 
#' The second check per-cell is based on the fine-tuning scores in \code{results$tuned.scores}, if available.
#' Here, the best and the next-best score at the final round of fine-tuning are reported for each cell.
#' We ignore any cell for which the fine-tuning score is not more than \code{min.diff.next} greater than the next best label.
#' The idea is to only report labels for which there is no ambiguity in assignment,
#' especially when some labels have similar scores because they are closely related (and thus easily confused).
#' Typical values of \code{min.diff.next} range between [0, 0.1], though check the caveats below.
#' 
#' For the per-label check, we identify cells that are small outliers in the distribution of scores for each label.
#' (This only includes cells that pass the per-cell check and uses only pre-fine-tuning scores.)
#' Specifically, cells that are more than \code{nmads} below the median score for each label are ignored.
#' This assumes that most cells are correctly assigned to their true label
#' and that cells of the same label have a smooth distribution of scores.
#' Thus, small outliers represent poor-quality assignments from a distinct subpopulation that should be removed.
#'
#' @section Comments on setting parameters:
#' The defaults for these parameters are largely arbitrary and chosen based on experience.
#' Smaller values for \code{min.diff.med} or \code{min.diff.next} and larger values for \code{nmads} will reduce the stringency of the pruning.
#' 
#' Default checks (\code{min.diff.med} and \code{nmads}) do not consider the effects of fine-tuning,
#' as scores are not comparable across different fine-tuning steps.
#' In situations involving a majority of labels with only subtle distinctions,
#' it is possible for the scores to be relatively similar but for the labels to be correctly assigned after fine-tuning.
#' In such cases, the default setting of \code{min.diff.med} may be too stringent.
#'
#' It is possible for the per-label score distribution to be multimodal yet still correct,
#' e.g., due to cells belong to subtypes when the (correct) label corresponds to the main type.
#' In such cases, the default \code{nmads} may be too stringent as it will remove minor subpopulations with low scores.
#'
#' The \code{min.diffnext} cutoff can be harmful in some applications involving highly related labels.
#' From a user perspective, any confusion between these labels may not be a problem as the assignment is broadly correct;
#' however, the best and next best scores will be very close and cause the labels to be unnecessarily discarded.
#' 
#' Note that decreasing \code{min.diff.med} may actually \emph{increase} the stringency of the per-label check, 
#' depending on whether the additional retained cells decrease the MAD.
#'
#' @author Aaron Lun and Daniel Bunis.
#'
#' @seealso
#' \code{\link{classifySingleR}}, to generate \code{scores}.
#' 
#' @examples
#' # Running the SingleR() example.
#' example(SingleR, echo=FALSE)
#'
#' summary(pruneScores(pred))
#'
#' # Less stringent:
#' summary(pruneScores(pred, min.diff.med=0))
#' summary(pruneScores(pred, nmads=5))
#' 
#' # More stringent:
#' summary(pruneScores(pred, min.diff.med=0.1))
#' summary(pruneScores(pred, nmads=2))
#' summary(pruneScores(pred, min.diff.next=0.1))
#' 
#' @export
#' @importFrom DelayedMatrixStats rowMedians 
#' @importFrom DelayedArray DelayedArray rowMaxs
#' @importFrom stats median mad
pruneScores <- function(results, min.diff.med = 0.05, nmads=3, min.diff.next = 0) {
    scores <- results$scores
    maxed <- rowMaxs(DelayedArray(scores))
    delta <- maxed - rowMedians(DelayedArray(scores))
    keep <- delta >= min.diff.med

    tune.scores <- results$tuning.scores
    if (!is.null(tune.scores)) {
        keep <- keep & (tune.scores$first - tune.scores$second) >= min.diff.next
    }

    best <- max.col(scores)
    by.label <- split(which(keep), best[keep])
    for (l in by.label) {
        current <- maxed[l]
        med <- median(current)
        MAD <- mad(current, center=med)
        keep[l] <- (current >= med - nmads * MAD)
    }

    !keep
}
